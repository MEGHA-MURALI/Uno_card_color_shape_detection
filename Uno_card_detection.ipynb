{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q keras==2.3.1\n",
    "#!pip install -q tensorflow-estimator==2.1\n",
    "#!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# initialize the path to the input directory containing our dataset\n",
    "# of images\n",
    "DATASET_PATH = \"C:/Users/97150/Desktop/Uno_card_color_shape_detection/Unocards_data\"\n",
    "# initialize the class labels in the dataset\n",
    "CLASSES = [\"Card_0\",\"Card_1\",\"Card_2\",\"Card_3\",\"Card_4\",\"Card_5\",\"Card_6\",\"Card_7\",\"Card_8\",\"Card_9\",\"Draw_2+\",\"Draw_4+\",\"Reverse\",\"Skip_Card\",\"Wild_Card\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size of the training, validation (which comes from the\n",
    "# train split), and testing splits, respectively\n",
    "TRAIN_SPLIT = 0.80\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the minimum learning rate, maximum learning rate, batch size,\n",
    "# step size, CLR method, and number of epochs\n",
    "MIN_LR = 1e-6\n",
    "MAX_LR = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "STEP_SIZE = 8\n",
    "CLR_METHOD = \"triangular\"\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the serialized model after training\n",
    "MODEL_PATH = os.path.sep.join([\"output\", \"unocard.model\"])\n",
    "# define the path to the output learning rate finder plot, training\n",
    "# history plot and cyclical learning rate plot\n",
    "LRFIND_PLOT_PATH = os.path.sep.join([\"output\", \"lrfind_plot.png\"])\n",
    "TRAINING_PLOT_PATH = os.path.sep.join([\"output\", \"training_plot.png\"])\n",
    "CLR_PLOT_PATH = os.path.sep.join([\"output\", \"clr_plot.png\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# grab the paths to all images in our dataset directory and initialize\n",
    "# our lists of images and class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = list(paths.list_images(DATASET_PATH))\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing data...\n"
     ]
    }
   ],
   "source": [
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    # extract the class label\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    # load the image, convert it to RGB channel ordering, and resize\n",
    "    # it to be a fixed 224x224 pixels, ignoring aspect ratio\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "print(\"[INFO] processing data...\")\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    " \n",
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=TEST_SPLIT, random_state=42)\n",
    "# take the validation split from the training split\n",
    "(trainX, valX, trainY, valY) = train_test_split(trainX, trainY,\n",
    "    test_size=VAL_SPLIT, random_state=84)\n",
    "# initialize the training data augmentation object\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# load the VGG16 network, ensuring the head FC layer sets are left\n",
    "# off\n",
    "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(len(CLASSES), activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "# compile our model (this needs to be done after our setting our\n",
    "# layers to being non-trainable\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "keras_callbacks = [EarlyStopping(monitor='val_loss',patience=3,mode='min',min_delta=0.0001),ModelCheckpoint('./Output',monitor='val_loss',save_best_only=True,mode='min')]\n",
    "opt = SGD(learning_rate=MAX_LR, momentum=0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/30\n",
      "168/168 [==============================] - 1173s 7s/step - loss: 4.3534 - accuracy: 0.2975 - val_loss: 1.4482 - val_accuracy: 0.5200\n",
      "Epoch 2/30\n",
      "168/168 [==============================] - 1157s 7s/step - loss: 1.8327 - accuracy: 0.4250 - val_loss: 1.0063 - val_accuracy: 0.6883\n",
      "Epoch 3/30\n",
      "168/168 [==============================] - 1143s 7s/step - loss: 1.4705 - accuracy: 0.5171 - val_loss: 0.8413 - val_accuracy: 0.7267\n",
      "Epoch 4/30\n",
      "168/168 [==============================] - 1135s 7s/step - loss: 1.3090 - accuracy: 0.5574 - val_loss: 0.6823 - val_accuracy: 0.7883\n",
      "Epoch 5/30\n",
      "168/168 [==============================] - 1144s 7s/step - loss: 1.1679 - accuracy: 0.6075 - val_loss: 0.5864 - val_accuracy: 0.8033\n",
      "Epoch 6/30\n",
      "168/168 [==============================] - 1141s 7s/step - loss: 1.0138 - accuracy: 0.6461 - val_loss: 0.4959 - val_accuracy: 0.8283\n",
      "Epoch 7/30\n",
      "168/168 [==============================] - 1139s 7s/step - loss: 0.9714 - accuracy: 0.6716 - val_loss: 0.4608 - val_accuracy: 0.8533\n",
      "Epoch 8/30\n",
      "168/168 [==============================] - 1143s 7s/step - loss: 0.8646 - accuracy: 0.6971 - val_loss: 0.3694 - val_accuracy: 0.8767\n",
      "Epoch 9/30\n",
      "168/168 [==============================] - 10622s 63s/step - loss: 0.8460 - accuracy: 0.7068 - val_loss: 0.3733 - val_accuracy: 0.8817\n",
      "Epoch 10/30\n",
      "168/168 [==============================] - 1169s 7s/step - loss: 0.7629 - accuracy: 0.7301 - val_loss: 0.3741 - val_accuracy: 0.8717\n",
      "Epoch 11/30\n",
      "168/168 [==============================] - 1176s 7s/step - loss: 0.7470 - accuracy: 0.7420 - val_loss: 0.3056 - val_accuracy: 0.9033\n",
      "Epoch 12/30\n",
      "168/168 [==============================] - 1192s 7s/step - loss: 0.6981 - accuracy: 0.7569 - val_loss: 0.2850 - val_accuracy: 0.9100\n",
      "Epoch 13/30\n",
      "168/168 [==============================] - 1183s 7s/step - loss: 0.6911 - accuracy: 0.7578 - val_loss: 0.2625 - val_accuracy: 0.9167\n",
      "Epoch 14/30\n",
      "168/168 [==============================] - 1178s 7s/step - loss: 0.6524 - accuracy: 0.7809 - val_loss: 0.2667 - val_accuracy: 0.9150\n",
      "Epoch 15/30\n",
      "168/168 [==============================] - 27132s 161s/step - loss: 0.6133 - accuracy: 0.7792 - val_loss: 0.2573 - val_accuracy: 0.9017\n",
      "Epoch 16/30\n",
      "168/168 [==============================] - 1192s 7s/step - loss: 0.5937 - accuracy: 0.7971 - val_loss: 0.2379 - val_accuracy: 0.9217\n",
      "Epoch 17/30\n",
      "168/168 [==============================] - 1191s 7s/step - loss: 0.5514 - accuracy: 0.8044 - val_loss: 0.1928 - val_accuracy: 0.9467\n",
      "Epoch 18/30\n",
      "168/168 [==============================] - 1196s 7s/step - loss: 0.5511 - accuracy: 0.8076 - val_loss: 0.1944 - val_accuracy: 0.9433\n",
      "Epoch 19/30\n",
      "168/168 [==============================] - 1196s 7s/step - loss: 0.5015 - accuracy: 0.8273 - val_loss: 0.2103 - val_accuracy: 0.9283\n",
      "Epoch 20/30\n",
      "168/168 [==============================] - 1195s 7s/step - loss: 0.4965 - accuracy: 0.8258 - val_loss: 0.1795 - val_accuracy: 0.9417\n",
      "Epoch 21/30\n",
      "168/168 [==============================] - 1191s 7s/step - loss: 0.4834 - accuracy: 0.8342 - val_loss: 0.2035 - val_accuracy: 0.9317\n",
      "Epoch 22/30\n",
      "168/168 [==============================] - 1200s 7s/step - loss: 0.4956 - accuracy: 0.8279 - val_loss: 0.1691 - val_accuracy: 0.9467\n",
      "Epoch 23/30\n",
      "168/168 [==============================] - 1209s 7s/step - loss: 0.4699 - accuracy: 0.8320 - val_loss: 0.1526 - val_accuracy: 0.9567\n",
      "Epoch 24/30\n",
      "168/168 [==============================] - 1241s 7s/step - loss: 0.4410 - accuracy: 0.8448 - val_loss: 0.1584 - val_accuracy: 0.9467\n",
      "Epoch 25/30\n",
      "168/168 [==============================] - 1205s 7s/step - loss: 0.4363 - accuracy: 0.8480 - val_loss: 0.1716 - val_accuracy: 0.9417\n",
      "Epoch 26/30\n",
      "168/168 [==============================] - 1247s 7s/step - loss: 0.4159 - accuracy: 0.8620 - val_loss: 0.1514 - val_accuracy: 0.9467\n",
      "Epoch 27/30\n",
      "168/168 [==============================] - 1263s 8s/step - loss: 0.3897 - accuracy: 0.8638 - val_loss: 0.1577 - val_accuracy: 0.9483\n",
      "Epoch 28/30\n",
      "168/168 [==============================] - 1200s 7s/step - loss: 0.3967 - accuracy: 0.8692 - val_loss: 0.1448 - val_accuracy: 0.9500\n",
      "Epoch 29/30\n",
      "168/168 [==============================] - 1191s 7s/step - loss: 0.3579 - accuracy: 0.8780 - val_loss: 0.1209 - val_accuracy: 0.9667\n",
      "Epoch 30/30\n",
      "168/168 [==============================] - 1203s 7s/step - loss: 0.3823 - accuracy: 0.8679 - val_loss: 0.1248 - val_accuracy: 0.9617\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "    validation_data=(valX, valY),\n",
    "    steps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Card_0       1.00      1.00      1.00       112\n",
      "      Card_1       1.00      0.98      0.99        91\n",
      "      Card_2       0.93      0.90      0.91       107\n",
      "      Card_3       0.99      0.91      0.95       123\n",
      "      Card_4       0.92      0.90      0.91        94\n",
      "      Card_5       0.97      0.96      0.97       103\n",
      "      Card_6       0.92      0.91      0.91        95\n",
      "      Card_7       0.90      1.00      0.95        88\n",
      "      Card_8       0.87      0.98      0.92       104\n",
      "      Card_9       1.00      0.98      0.99       102\n",
      "     Draw_2+       0.96      0.96      0.96       104\n",
      "     Draw_4+       1.00      1.00      1.00        91\n",
      "     Reverse       1.00      0.97      0.99       104\n",
      "   Skip_Card       0.97      1.00      0.98        87\n",
      "   Wild_Card       1.00      1.00      1.00        95\n",
      "\n",
      "    accuracy                           0.96      1500\n",
      "   macro avg       0.96      0.96      0.96      1500\n",
      "weighted avg       0.96      0.96      0.96      1500\n",
      "\n",
      "[INFO] serializing network to 'output\\unocard.model'...\n",
      "WARNING:tensorflow:From C:\\Users\\97150\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\97150\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: output\\unocard.model\\assets\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network and show a classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions.argmax(axis=1), target_names=CLASSES))\n",
    "# serialize the model to disk\n",
    "print(\"[INFO] serializing network to '{}'...\".format(MODEL_PATH))\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0,NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(TRAINING_PLOT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing video...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-85eca92a2f6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# queue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#preds = model.predict(frame)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# perform prediction averaging over the current history of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the video stream, pointer to output video file, and\n",
    "# frame dimensions\n",
    "from collections import deque\n",
    "print(\"[INFO] processing video...\")\n",
    "path =\"C:/Users/97150/Desktop/Uno_card_color_shape_detection/Card_0.mov\"\n",
    "vs = cv2.VideoCapture(path)\n",
    "# initialize the predictions queue\n",
    "Q = []\n",
    "\n",
    "(W, H) = (None, None)\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()\n",
    " \n",
    "    # if the frame was not grabbed, then we have reached the end\n",
    "    # of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    " \n",
    "    # if the frame dimensions are empty, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "        # clone the output frame, then convert it from BGR to RGB\n",
    "        # ordering and resize the frame to a fixed 224x224\n",
    "        output = frame.copy()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = frame.astype(\"float32\")\n",
    "\n",
    "        # make predictions on the frame and then update the predictions\n",
    "        # queue\n",
    "        #preds = model.predict(frame)\n",
    "        preds = model.predict(np.expand_dims(frame, axis=0))\n",
    "        Q.append(preds)\n",
    "        # perform prediction averaging over the current history of\n",
    "        # previous predictions\n",
    "        results = Q\n",
    "        i = np.argmax(results)\n",
    "        label = CLASSES[i]\n",
    "\n",
    "        # draw the activity on the output frame\n",
    "        text = \"detected card: {}\".format(label)\n",
    "        cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1.25, (0, 255, 0), 5)\n",
    "\n",
    "        cv2.imshow(\"Output\", output)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "vs.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
